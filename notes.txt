Hierachical, Each omic vs early fusion, training procedure, SOM ? 

Alternative methods of Population Coding:
- Take the max *per class* instead of per neuron, 
  UNLESS that neuron is already assigned?
    N0: [100, 50] = C1
    N1: [120, 60] = C0
    -- Only works if N# = C#

- Subtract value of every other class per neuron, then take max
    N0: [110, 40] = [70] = C0   #40 - 110 = -70 = C0
    N1: [120, 70] = [50] = C1   #70 - 120 = -50 = C1

- Divide by the number of shown samples for each class
    S0 = 100, S1 = 10
    N0: [200, 40] = [2, 4] = C1
    N1: [4000, 100] = [40, 10] = C0

Problem with all of these is that SMOTE is still necessary.
If the neuron never gets a chance to respond to a 1, it will have 0 spikes for the minority class.

Argmax is also not great, but these metrics just kick the can down the road.

Strength of response to individual image?
 Currently that information is discarded by argmax, what if we use it as a weight?
    e.g. neuron 4 reacted most strongly, but that one reacts strongly to everything and has weak class seperation.
    but the sum of neurons 0,1,2 are stronger than neuron 4 and they all predict the opposite class with greater seperation.
    Could even do a linear regression of responses to targets lol

Population Vector Decoder - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3367001/
"For the population vector decoder the responses of all neurons were vector summed with an orientation equal to the neuronâ€™s preferred orientation before adaptation."

Temporal Winner-Takes-All - https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000286

Class seperation is an extremely valuable property of the network. 
We want the responses for each neuron to be as distinct as possible for different classes, eg responding very highly to 1 and not at all to 0.
Could even 'prune' neurons with low distinction?


Reframe around Problems with Population Coding:
- Class imbalance
    -- Plot Recall/F1 against SMOTE%
    -- Plot % of neurons assigned to each class against SMOTE%

- Training paradigm
    -- Describe problems, small test set = always correct
    -- Move labels to training phase

    -- Delve into the fact that this is essentially supervised?
        * We have to embed the number of classes into the model as a pre-requisit,
          because if classes > neurons then it will never predict those classes.
          Worse, even if classes =< neurons then we can't guarentee that each class will be represented.
        * Labels must then be presented to the network in order for it to be useful...

        When you show the network an image, if you have K_o=8, it spits out [0, 3, 0, 0, 0, 0, 0, 0].
        Next image, it spits out [0, 0, 4, 0, 0, 0, 0, 0]. Great, what do we do with that?

        Each image will give an output of some arbitrary array of numbers, 
        the length of which does not correspond to the number of classes.

        So like, you could just take the max of that array as your class, 
        but that neuron may actually respond more to a different class (a la the email). 
        Each neuron would effectively have a random number as the response value per image.
        So it's really not a "clustering" algorithm, it produces a scalar output not a class.
        You'd have to then apply a secondary clustering algorithm to group the responses by density.
        Basically what you've done is feature engineering lmao

          


